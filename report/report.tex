\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[us,nodayofweek]{datetime}
\usepackage[style=alphabetic,maxbibnames=99,giveninits]{biblatex}
\addbibresource{references.bib}
\graphicspath{{./images/}}

\title{Super Mario World AI}
\author{Siebren Cosijn}
\newdate{date}{31}{08}{2022}
\date{\displaydate{date}}
\begin{document}
    \maketitle

    \section{Introduction}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\textwidth]{start-screen}
    \end{figure}
    My graduate work is based on the popular game ``Super Mario World''.
    Super Mario is a game that was developed by Nintendo for their SNES\footnote{Super Nintendo Entertainment System}.
    The aim of the game is to complete a number of levels by reaching the â€œgoalpost" for each level.
    During each level, which consists of different worlds, obstacles and enemies must be avoided and eliminated.
    The player has a console to control the game.
    Very simply said, the intention during this graduate work is to replace the player and console with an AI model.

    For the elaboration of this work I have used the following components and techniques.
    \begin{itemize}
        \item Reinforcement learning
        \item OpenAI Gym\footnote{\url{https://github.com/openai/gym}} \cite{brockman2016openai} - RL library
        \item Gym Retro\footnote{\url{https://github.com/openai/retro/}} - Game integration for Gym
    \end{itemize}
    These different components are discussed in more detail later in this document.

    \section{Goals}
    The ultimate goal of this project is to train an agent so that he can complete some levels of Super Mario World.
    The aim is to make the agent learn and deal with the obstacles presented in the game.
    Obstacles can consist of obstacles or enemies that must be defeated or dodged.
    \begin{itemize}
        \item Train an agent to complete level(s) of the game
        \item Test agent on unseen levels
    \end{itemize}
    The goal is to train an agent to complete level(s) of the game.
    Look at how the agent learns to deal with obstacles presented by the game.
    Test the agent on new/unseen data (in the form of levels the agent hasn't trained on).

    \section{Reinforcement Learning}
    Reinforcement learning loop
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{AE_loop}
        \caption{The agent-environment loop. \textbf{Source:} \url{https://www.gymlibrary.ml/_images/AE_loop.png}}
        \label{fig:loop}
    \end{figure}
    \begin{itemize}
        \item agent = RL model
        \item environment = game
        \item action = button input
        \item state = current frame
        \item reward = decide if action was good
    \end{itemize}
    In this case an action means pressing buttons on the controller.
    The observation is the next frame of the game after the action has been executed.
    The reward is based on how good or bad the effect of the action was on the environment according to the reward function.

    \section{Preprocessing}
    Before we can train our agent a number of preprocessing steps are necessary.
    This includes a transformation of both the action and observation space.
    Most of these steps are based on papers discussing the Arcade Learning Environment (ALE) \cite{bellemare2013arcade,machado2018revisiting}.
    The ALE is a framework that allows people to develop AI agents for Atari\footnote{Atari 2600, originally known as Atari VCS (Video Computer System)} games and is commonly used to benchmark reinforcement learning algorithms.
    Even though the ALE was created for Atari games, most of the preprocessing steps used are also applicable to the SNES (and Super Mario World).
    The following steps are implemented as wrappers around the Gym environment.
    \subsection{Action Space}
    By default, the gym retro environment for Super Mario World uses a MultiBinary\footnote{\url{https://www.gymlibrary.ml/content/spaces/\#multibinary}} space with 12 elements (buttons).
    Each actions contains a value for every button in the action space where 0 (not pressed) or 1 (pressed). This results in $2^{12}=4096$ possible combinations.
    Sticking to the default MultiBinary action space creates three problems:
    \begin{itemize}
        \item Large action space: agent takes longer to find good actions during exploration
        \item Invalid combinations: some combinations of key presses are invalid (i.e. left and right at the same time)
        \item Useless buttons: some buttons are used to control the game menu or pause the game and have no effect on the actual gameplay
    \end{itemize}
    \textbf{TODO.}
    One button of each cluster can be pressed at the same time.
    Two discrete action spaces with respectively 5 and 4 actions.
    MultiDiscrete\footnote{\url{https://www.gymlibrary.ml/content/spaces/\#multidiscrete}} action space.
    \begin{itemize}
        \item movement keys: none, left, right, down, up
        \item special keys: none, A (spin), B (jump), X = Y (run)
        \item $5*4=20$ valid actions 
    \end{itemize}
    \begin{figure}[H]
        \centering
        \includegraphics[width=.75\textwidth]{snes-controller-annot}
        \caption{SNES controller}
        \label{fig:snes-controller}
    \end{figure}
    \subsection{Observation Space}
    \subsubsection{Transform observation}
    To reduce the memory requirement, each observation is rescaled from 256x224 pixels to 84x84 pixels.
    The color is also converted to grayscale.
    Figure \ref{fig:transformation} shows the result of this transformation.
    Lowering the memory requirement significantly reduces the training time of the agent.
    %TODO fix axes
    \begin{figure}[H]
        \centering
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[height=6cm]{original_crop}
            \caption{Original}
            \label{fig:sub1}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[height=6cm]{grayscale_crop}
            \caption{Transformed}
            \label{fig:sub2}
        \end{subfigure}
        \caption{Pixel Transformation}
        \label{fig:transformation}
    \end{figure}
    \subsubsection{Skip frames}
    SNES games run at 60 FPS (frames per second), which means there is very little change from frame to frame.
    As such, it is unnecessary to input an action each frame (also human players do not press 60 buttons each second).
    To create the effect of ``skipping'' frames we repeat each action over four frames and sum the accumulated rewards.
    This can be seen as the equivalent of a human player holding the button for a few frames.
    This change brings the agent closer to human behaviour and as an additional bonus speeds up the training of the agent.
    \subsubsection{Stack frames}
    The agent is trained on pixels, so each observation can be seen as some sort of screenshot of the game state.
    By using a single frame, the agent cannot perceive the direction or velocity of movement (is Mario going up or down?).
    To solve this we ``stack'' four subsequent observations on top of each other and use the resulting observation for training.
    Each observation is placed in a queue and concatenated with the previous three most recent observations.
    On figure \textbf{TODO: refer to figure by label} we can see based on the last four observations that Mario is jumping to the right.
    \begin{figure}[H]
        \centering
        \includegraphics[width=.6\textwidth]{stacked}
    \end{figure}
    \subsection{Other}
    \subsubsection{Sticky Actions}
    \begin{itemize}
        \item AI needs to learn a policy that will work on other levels, not memorize one path in one level
        \item add stochasticity
        \item 25\% chance to repeat the previous action
    \end{itemize}
    \subsubsection{Episodic Life}
    \begin{itemize}
        \item Reset environment when a life is lost
        \item 5 lives, return to menu on death
        \item Prevent agent getting stuck in game menu
    \end{itemize}

    \section{Reward Shaping}
    \begin{itemize}
        \item ideal reward = +1 level complete (too sparse)
        \item end of level is always on the right side
        \item reward for moving right
        \item bonus for checkpoint/end of level
    \end{itemize}
    reward function based on https://github.com/Kautenja/gym-super-mario-bros/
    velocity: the difference in the agent's x position between states

    velocity = x1 - x0

    x0 is the x position before the step

    x1 is the x position after the step

    \begin{itemize}
        \item moving right $\Leftrightarrow$ $v > 0$
        \item moving left $\Leftrightarrow$ $v < 0$
        \item not moving $\Leftrightarrow$ $v = 0$
    \end{itemize}
    bonus:
    \begin{itemize}
        \item bonus of 'max reward'/2 for reaching checkpoint
        \item bonus of 'max reward' for reaching end of level
    \end{itemize}
    no penalty for death as this causes the agent to be too cautious (not enough exploration)

    total reward is clipped between 'min reward' and 'max reward' (default -15, 15)

    \section{Algorithm}
    \begin{itemize}
        \item Model-Free RL vs Model-Based RL
        \item Policy Optimization vs Q-Learning
    \end{itemize}
    PPO:
    \begin{itemize}
        \item Proximal Policy Optimization (PPO)\cite{schulman2017proximal}
        \item Policy based algorithm
        \item Stable Baselines library\footnote{\url{https://github.com/DLR-RM/stable-baselines3}}
    \end{itemize}
    \subsection{Hyperparameters}
    The following hyperparameters for PPO were used to train the agent.
    \begin{itemize}
        \item learning\_rate = $1e^{-4}$
        \item n\_steps = 512
        \item batch\_size = 512
        \item n\_epochs = 2
        \item clip\_range = 0.1
        \item ent\_coef = 0.001
    \end{itemize}

    \section{Training \& Results}
    \subsection{Model A}
    Trained for 25 million time steps on the level 'YoshiIsland2'.
    Most basic level (avoid enemies, jump over pit).
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{plot_model_a.png}
    \end{figure}

    \printbibliography
\end{document}